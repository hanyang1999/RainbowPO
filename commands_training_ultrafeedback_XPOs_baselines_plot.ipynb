{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Llama-3-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF baselines (Deepspeed Zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF on Llama-3-Instruct (DPO $\\beta$=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change learning rate \n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 5e-7 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_dpo_beta_0.05_lr_0.5_max_token_2048\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 2048 \\\n",
    "    --max_prompt_length 1800 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --beta 0.05 \\\n",
    "    --loss_type \"sigmoid\" \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF on Llama-3-Instruct (IPO $\\beta$=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPO with 2048 \n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 1\\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 5e-7 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_ipo_beta_0.01_lr_0.5_max_token_2048\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 2048 \\\n",
    "    --max_prompt_length 1800 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --beta 0.01 \\\n",
    "    --loss_type \"ipo\" \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF on Llama-3-Instruct (ORPO $\\beta$=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO on Llama-3-Instruct\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 8 examples/scripts/orpo_zero3.py \\\n",
    "    --dataset=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_orpo_beta_0.1\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 2048 \\\n",
    "    --max_prompt_length 1800 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --beta 0.005 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF on Llama-3-Instruct (SimPO $\\beta$=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimPO on Llama-3-Instruct\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_simpo_beta_10_hadv_0.3_max_token_2048\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 2048 \\\n",
    "    --max_prompt_length 1800 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --beta 10 \\\n",
    "    --length_normalization True \\\n",
    "    --home_advantage 0.3 \\\n",
    "    --loss_type \"sigmoid\" \\\n",
    "    --reference_free True \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mallows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_mallows_dpo_beta_10_normalized_max_token_2048\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 2048 \\\n",
    "    --max_prompt_length 1800 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --loss_type \"mallows-dpo\" \\\n",
    "    --num_train_epochs 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!NCCL_P2P_DISABLE=1 accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 7 examples/scripts/kto_zero3.py \\\n",
    "    --dataset_name=\"/home/hanyang/RainbowPO/examples/datasets/UltraFeedback_armorm_kto_trl\" \\\n",
    "    --model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 5e-7 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 10 \\\n",
    "    --output_dir=\"models_rlhf/Llama3-Instruct_armorm_kto_beta_0.05_lr_5e-7_max_token_2048\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 2048 \\\n",
    "    --max_prompt_length 1800 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --beta 0.05 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!NCCL_P2P_DISABLE=1 accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 7 examples/scripts/cpo_zero3.py \\\n",
    "    --dataset=\"/home/hanyang/RainbowPO/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 10 \\\n",
    "    --output_dir=\"models_rlhf/Llama3-Instruct_armorm_kto_beta_0.05_lr_5e-7_max_token_2048\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 2048 \\\n",
    "    --max_prompt_length 1800 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --beta 0.05 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RainbowPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_simpo_beta_10_hadv_0.3_max_token_2048\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 2048 \\\n",
    "    --max_prompt_length 1800 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --beta 10 \\\n",
    "    --length_normalization True \\\n",
    "    --home_advantage 0.3 \\\n",
    "    --loss_type \"mallows_dpo\" \\\n",
    "    --if_mixing_alpha True \\\n",
    "    --mixing_alpha 0.5 \\\n",
    "    --reference_free False \\\n",
    "    --num_train_epochs 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
