{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Llama-3-8B-SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Warning: Using transformer version in DPO will lead to errors of loading Llama3\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"<some path>/model_zoo/Meta-Llama-3-8B\",device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"<some path>/model_zoo/Meta-Llama-3-8B\",device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7d9b8719824fdf8e56585d479c3d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_sft = AutoTokenizer.from_pretrained(\"<some path>/trl/models/Llama3_sft_UltraChat_SimPO/checkpoint-4872\",device_map=\"auto\")\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(\"<some path>/trl/models/Llama3_sft_UltraChat_SimPO/checkpoint-4872\",device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68e6dcdb9e64eabb8b9ba98c516f749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_instruct = AutoTokenizer.from_pretrained(\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\",device_map=\"auto\")\n",
    "model_instruct = AutoModelForCausalLM.from_pretrained(\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\",device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fff351f5fbe4caaa1add692bd142089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_dpo = AutoTokenizer.from_pretrained(\"<some path>/trl/models/Llama3-Instruct_UltraFeedback_dpo\",device_map=\"auto\")\n",
    "model_dpo = AutoModelForCausalLM.from_pretrained(\"<some path>/trl/models/Llama3-Instruct_UltraFeedback_dpo\",device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sft.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset_path = \"<some path>/trl/examples/datasets/UltraFeedback_trl\"\n",
    "ds = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Teacher:In this task, you're given reviews from Amazon's food products and a summary of that review. Your task is to classify whether the given summary matches the original review. Generate \"True\" if the given review and its summary match, otherwise generate \"False\".\n",
      "Teacher: Now, understand the problem? Solve this instance: Reviews: product was shipped in orginal box placed inside a larger box, no packing was used. Bulbs were broken when they arrived. \n",
      " Summary: product was shipped in orginal box placed inside a larger ...\n",
      "Student:<|eot_id|>\n",
      "---------------------------------------------------------------------------------------------------\n",
      "dataset_response (chosen):\n",
      " \n",
      " <|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "True<|eot_id|> \n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "\n",
    "input = ds[\"test\"][index][\"chosen\"][:-1]\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "print(tokenizer_dpo.apply_chat_template(input,tokenize=False))\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "dataset_chosen_response = ds[\"test\"][index][\"chosen\"][-1:]\n",
    "\n",
    "print(f\"dataset_response (chosen):\\n \\n {tokenizer_dpo.apply_chat_template(dataset_chosen_response,tokenize=False)} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "#model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer_sft,\n",
    "    # model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    # device=\"cuda\",\n",
    ")\n",
    "\n",
    "pipe_sft = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_sft,\n",
    "    tokenizer=tokenizer_sft,\n",
    "    # model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    # device=\"cuda\",\n",
    ")\n",
    "\n",
    "pipe_instruct = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_instruct,\n",
    "    tokenizer=tokenizer_instruct,\n",
    "    # model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    # device=\"cuda\",\n",
    ")\n",
    "\n",
    "pipe_dpo = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_dpo,\n",
    "    tokenizer=tokenizer_dpo,\n",
    "    # model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    # device=\"cuda\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipe.tokenizer.eos_token_id,\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs= pipe(\n",
    "    input,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "outputs_sft = pipe_sft(\n",
    "    input,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "outputs_instruct = pipe_instruct(\n",
    "    input,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "outputs_dpo = pipe_dpo(\n",
    "    input,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Teacher:In this task, you're given reviews from Amazon's food products and a summary of that review. Your task is to classify whether the given summary matches the original review. Generate \"True\" if the given review and its summary match, otherwise generate \"False\".\n",
      "Teacher: Now, understand the problem? Solve this instance: Reviews: product was shipped in orginal box placed inside a larger box, no packing was used. Bulbs were broken when they arrived. \n",
      " Summary: product was shipped in orginal box placed inside a larger ...\n",
      "Student:<|eot_id|>\n",
      "---------------------------------------------------------------------------------------------------\n",
      "dataset_response (chosen):\n",
      " \n",
      " <|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "True<|eot_id|> \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Response by Llama3 after SFT: \n",
      " \n",
      " The summary does not match the original review. The summary only states that the product was shipped in its original box, while the review also mentions that the product was placed inside a larger box and that no packing was used. Additionally, the review mentions that bulbs were broken upon arrival, which is not mentioned in the summary. Therefore, the answer is \"False\".\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Response by Llama3 Instruct: \n",
      " \n",
      " True\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Response by Llama3 Instruct (after DPO): \n",
      " \n",
      " True\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "\n",
    "input = ds[\"test\"][index][\"chosen\"][:-1]\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "print(tokenizer_dpo.apply_chat_template(input,tokenize=False))\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "dataset_chosen_response = ds[\"test\"][index][\"chosen\"][-1:]\n",
    "\n",
    "print(f\"dataset_response (chosen):\\n \\n {tokenizer_dpo.apply_chat_template(dataset_chosen_response,tokenize=False)} \")\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "print(f\"Response by Llama3 before SFT:\\n \\n {assistant_response} \")\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "assistant_response_sft = outputs_sft[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "print(f\"Response by Llama3 after SFT: \\n \\n {assistant_response_sft}\")\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "assistant_response_instruct = outputs_instruct[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "print(f\"Response by Llama3 Instruct: \\n \\n {assistant_response_instruct}\")\n",
    "\n",
    "print(dash_line)\n",
    "\n",
    "assistant_response_dpo = outputs_dpo[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "print(f\"Response by Llama3 Instruct (after DPO): \\n \\n {assistant_response_dpo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ds[\"test\"][100:110][\"chosen\"]\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>chosen</th>\n",
       "      <th>sft</th>\n",
       "      <th>instruct</th>\n",
       "      <th>dpo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>Equilibrium price and quantity for robots:\\nQ ...</td>\n",
       "      <td>**Equilibrium Price and Quantity for Each Toy:...</td>\n",
       "      <td>To find the equilibrium price and quantity for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>The question asks about a specific operation, ...</td>\n",
       "      <td>A: During his time in the United States, he wr...</td>\n",
       "      <td>A: When was the subject of White Lies born?\\nP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>- Upon clicking the button, a modal window sho...</td>\n",
       "      <td>Here are the instructions to create a Lightnin...</td>\n",
       "      <td>Here are the steps to create a lightning compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>Code-breaking played a crucial role in the Bat...</td>\n",
       "      <td>Code-breaking played a crucial role in the Bat...</td>\n",
       "      <td>Code-breaking played a crucial role in the Bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>I do not have access to visual images. But, I ...</td>\n",
       "      <td>The EE Buck, HIL, and test bench are all conce...</td>\n",
       "      <td>EE Buck, HIL, and test bench are all terms use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>Online courses can provide individuals seeking...</td>\n",
       "      <td>Online courses can be a valuable resource for ...</td>\n",
       "      <td>Online courses can be a valuable resource for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>Acknowledged.</td>\n",
       "      <td>Acknowledged</td>\n",
       "      <td>Acknowledged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>Sure, let's start an AI project! What is the o...</td>\n",
       "      <td>I'd be happy to start an AI project with you. ...</td>\n",
       "      <td>Exciting! I'd be delighted to help you start a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>Diagnosis: Anxiety disorder\\nTreatment plan: I...</td>\n",
       "      <td>**Diagnosis:** Social Anxiety Disorder (SAD)\\n...</td>\n",
       "      <td>Diagnosis: Social Anxiety Disorder (SAD)\\n\\nTr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>Dear Chretien,\\n\\nI am glad to hear that we ha...</td>\n",
       "      <td>Here is a reviewed and corrected version of th...</td>\n",
       "      <td>Here is a revised version of the email that in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "1  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "2  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "3  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "4  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "5  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "6  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "7  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "8  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "9  <|begin_of_text|><|start_header_id|>user<|end_...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "1  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "2  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "3  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "4  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "5  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "6  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "7  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "8  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "9  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "\n",
       "                                                 sft  \\\n",
       "0  Equilibrium price and quantity for robots:\\nQ ...   \n",
       "1  The question asks about a specific operation, ...   \n",
       "2  - Upon clicking the button, a modal window sho...   \n",
       "3  Code-breaking played a crucial role in the Bat...   \n",
       "4  I do not have access to visual images. But, I ...   \n",
       "5  Online courses can provide individuals seeking...   \n",
       "6                                      Acknowledged.   \n",
       "7  Sure, let's start an AI project! What is the o...   \n",
       "8  Diagnosis: Anxiety disorder\\nTreatment plan: I...   \n",
       "9  Dear Chretien,\\n\\nI am glad to hear that we ha...   \n",
       "\n",
       "                                            instruct  \\\n",
       "0  **Equilibrium Price and Quantity for Each Toy:...   \n",
       "1  A: During his time in the United States, he wr...   \n",
       "2  Here are the instructions to create a Lightnin...   \n",
       "3  Code-breaking played a crucial role in the Bat...   \n",
       "4  The EE Buck, HIL, and test bench are all conce...   \n",
       "5  Online courses can be a valuable resource for ...   \n",
       "6                                       Acknowledged   \n",
       "7  I'd be happy to start an AI project with you. ...   \n",
       "8  **Diagnosis:** Social Anxiety Disorder (SAD)\\n...   \n",
       "9  Here is a reviewed and corrected version of th...   \n",
       "\n",
       "                                                 dpo  \n",
       "0  To find the equilibrium price and quantity for...  \n",
       "1  A: When was the subject of White Lies born?\\nP...  \n",
       "2  Here are the steps to create a lightning compo...  \n",
       "3  Code-breaking played a crucial role in the Bat...  \n",
       "4  EE Buck, HIL, and test bench are all terms use...  \n",
       "5  Online courses can be a valuable resource for ...  \n",
       "6                                       Acknowledged  \n",
       "7  Exciting! I'd be delighted to help you start a...  \n",
       "8  Diagnosis: Social Anxiety Disorder (SAD)\\n\\nTr...  \n",
       "9  Here is a revised version of the email that in...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_prompts = []\n",
    "chosen_responses = []\n",
    "llama3_sft_responses = []\n",
    "llama3_instruct_responses = []\n",
    "llama3_dpo_responses = []\n",
    "\n",
    "for dialogue in inputs:\n",
    "    chosen_responses.append(tokenizer_dpo.apply_chat_template(dialogue[-1:],tokenize=False))\n",
    "\n",
    "    input = dialogue[:-1]\n",
    "\n",
    "    input_prompts.append(tokenizer_dpo.apply_chat_template(dialogue[:-1],tokenize=False))\n",
    "\n",
    "    outputs_sft = pipe(\n",
    "        input,\n",
    "        max_new_tokens=100,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "    outputs_instruct = pipe_instruct(\n",
    "        input,\n",
    "        max_new_tokens=100,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "    outputs_dpo = pipe_dpo(\n",
    "        input,\n",
    "        max_new_tokens=100,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    \n",
    "    llama3_sft_responses.append(outputs_sft[0][\"generated_text\"][-1][\"content\"])\n",
    "    llama3_instruct_responses.append(outputs_instruct[0][\"generated_text\"][-1][\"content\"])\n",
    "    llama3_dpo_responses.append(outputs_dpo[0][\"generated_text\"][-1][\"content\"])\n",
    "\n",
    "all_examples = list(zip(input_prompts, chosen_responses, llama3_sft_responses, llama3_instruct_responses, llama3_dpo_responses))\n",
    "\n",
    "df = pd.DataFrame(all_examples,columns=[\"input\",\"chosen\",\"sft\",\"instruct\",\"dpo\"])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
