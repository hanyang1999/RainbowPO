{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF on Llama3-Instruct (DPO $\\beta$=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO on Llama-3-Instruct\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf_lora/Llama3-Instruct_armorm_LoRA_r_32_a_64_dpo_beta_0.05_retest\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --use_peft \\\n",
    "    --lora_r=32 \\\n",
    "    --lora_alpha=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO on Llama-3-Instruct\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf_lora/Llama3-Instruct_armorm_LoRA_r_128_a_256_dpo_beta_0.1\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --use_peft \\\n",
    "    --lora_r=128 \\\n",
    "    --lora_alpha=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length normalization\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf_lora/Llama3-Instruct_armorm_LoRA_r_128_a_256_dpo_beta_0.1_normalized\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2'\\\n",
    "    --use_peft \\\n",
    "    --lora_r=128 \\\n",
    "    --lora_alpha=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF on Llama3-Instruct (DPO $\\beta$=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO on Llama-3-Instruct\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf_lora/Llama3-Instruct_armorm_LoRA_r_128_a_256_dpo_beta_0.05\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --use_peft \\\n",
    "    --lora_r=128 \\\n",
    "    --lora_alpha=256 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change learning rate\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 5e-7 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_LoRA_r_128_a_256_dpo_beta_0.05_lr_0.5\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --use_peft \\\n",
    "    --lora_r=128 \\\n",
    "    --lora_alpha=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length normalization\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_LoRA_r_32_a_64_dpo_beta_0.05_normalized\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --use_peft \\\n",
    "    --lora_r=32 \\\n",
    "    --lora_alpha=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF on Llama3-Instruct (DPO $\\beta$=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO on Llama-3-Instruct\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_LoRA_r_128_a_256_dpo_beta_0.01\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --use_peft \\\n",
    "    --lora_r=128 \\\n",
    "    --lora_alpha=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length normalization\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_LoRA_r_32_a_64_dpo_beta_0.01_normalized\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --use_peft \\\n",
    "    --lora_r=32 \\\n",
    "    --lora_alpha=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further length normalization and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_LoRA_r_32_a_64_dpo_beta_10_normalized\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2'\\\n",
    "    --use_peft \\\n",
    "    --lora_r=32 \\\n",
    "    --lora_alpha=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home advantage with different values (2, dpo)\n",
    "!accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero1.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_LoRA_r_32_a_64_dpo_beta_0.05_normalized_hadv_0.001\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2'\\\n",
    "    --use_peft \\\n",
    "    --lora_r=32 \\\n",
    "    --lora_alpha=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XPOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --num_processes 8 examples/scripts/dpo_zero3.py \\\n",
    "    --dataset_name=\"<some path>/trl/examples/datasets/UltraFeedback_armorm_trl\" \\\n",
    "    --model_name_or_path=\"<some path>/model_zoo/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1.0e-6 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50000 \\\n",
    "    --output_dir=\"<some path>/trl/models_rlhf/Llama3-Instruct_armorm_LoRA_r_128_a_256_rainbowpo_beta_0.01\" \\\n",
    "    --optim adamw_torch \\\n",
    "    --max_length 1024 \\\n",
    "    --max_prompt_length 1000 \\\n",
    "    --seed 42 \\\n",
    "    --bf16 \\\n",
    "    --warmup_steps 150 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --attn_implementation 'flash_attention_2' \\\n",
    "    --use_peft \\\n",
    "    --lora_r=128 \\\n",
    "    --lora_alpha=256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
